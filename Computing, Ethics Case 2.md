The reference informationï¼š
FLAWED AI MAKES ROBOTS RACIST, SEXIST, Jill Rosen , Jun 21, 2022
https://hub.jhu.edu/2022/06/21/flawed-artificial-intelligence-robot-racist-sexist/

The article is mainly about how the flawed artificial intelligence models combined with the inaccurate database cause some biased activities of robots. A group of researchers from several universities built a robot operating system using an accepted and widely used model, but that kind of robot system may not be safe for marginalized groups. Such artificial intelligence models and datasets are available free on the Internet. They train the models with notoriously biased content so the robot system learns about how to be a racist. The robot puts toxic stereotypes on people such as they label women as homemakers and black people as criminals. Furthermore, if you ask the robot to put the criminal into the box, it just predicts someone as a criminal by their face. The robot system just reenacting human stereotypes as well as they have learned. The researcher of the article suggests that we may need systematic changes to the research and business practices, otherwise, future machines may harm marginalized groups.

The ethical issue raised by the article is the robot's learning content and ways. The original algorithm delivers toxic and radical content that prompts robots to learn and act racist and sexist. They suggest changing the system of research and business practices of the artificial intelligence models and the datasets. The ethical problem may be similar to the education problem about whether people have the right to choose what they want to see and learn. The government and the researcher may simply solve the problem by banning radical content. They also could develop an algorithm that regulates the actions of the robots so they are coded to do or not to do. But the problem is if we arbitrarily decide the content and algorithm of the robot, we seem to limit the horizon of the robot. And the flexibility of the robot system would be impaired. Furthermore, we need an organization to formulate a rule for the robot's algorithm and learning content.

The ethical issue is far from being handled. The article suggests that we need a systematic change to research and business practices but does not describe it in detail. I argue that we may take the Utilitarianism way and the Social Contract Theory to solve it more practically. The algorithm improvement of the robot system should not be banned or impaired, but we could add a reward and punishment system to the robot so it learns to act more conform to social norms and forms a view to judge the radical content. This solution balanced the interest of marginalized groups and the future of the robot algorithm. But the problem is how to determine which act is biased and which conforms to social norms. To solve it we may need a committee to decide such social norms and add them to the robot's reward and punishment system. This may be kind of limited to our social ethical values but it is the most harmless way. The robot could still develop and prosper in a broader space with a tiny limitation. The marginalized groups could appeal to their new claims to adjust the norm of robots of the committee. In conclusion, this solution to the ethical issue is both flexible and maximizes the benefits.
