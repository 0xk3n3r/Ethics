The reference information for the article :
title: What a machine learning tool that turns Obama white can (and canâ€™t) tell us about AI bias, 
author: James Vincent,
published: The Verge, 
link:https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias

summary of the content of the article:
The article researches the algorithm called Pulse that determines and generates the face even from a pixelated image. The Pulse algorithm uses a low-resolution face of Obama generates a white one which implies its deep-rooted racial bias. Subsequently, people find such an algorithm generated every resulting face distinctly white. The Pulse algorithm uses the StyleGAN algorithm created by NVIDIA. The algorithm generates the high-res version of pixelated inputs and remains the same when turns to the original pixelated version. However, the creator of Pulse admitted that the algorithm mainly generates faces with Caucasian features, but he shirked his responsibility to the dataset of StyleGAN. Such racial discrimination problem is prevalent in machine learning, especially in the facial recognition field. The data used to train the algorithm is mainly the white man demographic, which leads to racial bias performance of the algorithm. Some critics argue that people of color are not outliers and edge cases that creators can forget. However, the diversity in data may not be enough to solve the racism problem of the algorithm.

The ethical issue of the article is racial discrimination caused by the facial-generating algorithm. The synthetic faces generated by the Pulse algorithm demonstrate the preference toward white people by whom AI research is dominated. The solution to racial discrimination in the article is to use some fair datasets, but such datasets may not be fair enough for some data are predominantly white people. The algorithm itself does not have any adjustment for racial or gender disparities. Such a facial-generating algorithm reflects the implicit biases of the machine learning field. However, people who advocate the algorithm argue that selected data and adjusted algorithms may lose their potential. The problem may turn to weighing between the velocity of the machine learning development and the implicit racial bias among the facial-generating algorithm. The creator of the algorithm could take precautions such as using more representative data to prevent the system from impairing the interest of certain groups, but the future of the algorithm may be fuzzy.

According to the article, the ethical issue is not being handled perfectly. The creator of the facial-generating algorithm may replace the data with more "fair" data, but such data is still not complete, balanced, or selected appropriately. Additionally, the source of algorithmic racial bias does not simply come from the dataset but also comes from the algorithm itself and its creators. The creator of the facial-generating algorithm may unconsciously allocate too much weight to the model of specific ethnic groups. The data itself may be unbiased, but such ethnic affinities algorithm may select specific white faces that it generates. After the racial bias algorithm is developed and refined over the years, it may amplify the bias. So the primary factor of the ethical problem solution is to balance the algorithm's preference. The team of facial-generating algorithm creators must be diverse to avoid such a white domine situation, and the algorithm must design accountability and fairness. Such alter in the facial-generating algorithm may cause technical regress, but the mitigation of racial discrimination benefit is huge and uncountable. So we have to extract the embedding of racial bias in the machine learning field. The incremental racial bias in algorithms may turn into social division in the future. We may sacrifice the present benefit and convenience of technology to achieve a diverse and prolific tech future.
