According to the current imperfect and flaws state already shown in facial recognition software, I argue that such software should be implemented in non-private areas and must have people's consent for the facial recognition algorithm application could violate personal privacy because it is personal biometric information. Companies and governments could use it to identify their employees. People could use facial recognition software to access their less important properties such as phones. 
Facial recognition software should not used in the law enforcement initiative search and locate field for now because law enforcement may enhance the policy of accessing the database and the system, but they have not taken the problem of systemically biased algorithms. The inaccuracy recognition algorithm may cause errors in incrimination. If malicious actors recognize the flaw in the facial recognition system, they could take advantage of the flaw by identity theft. The more error algorithms were made, the more probability the system would be hacked. The facial recognition system could be fooled by masks or even elaborately synthetic faces. 

In the case of facial recognition software reaching its singularity to be flawless, such a technique could be used in the private properties field as biometric authentication and even implemented in government departments. Such a biometric identification technique is speedy, unique, and safer than slow traditional material identification techniques. Public places such as airports and the metro could use it to recognize criminals and threats but only as a passive defense system.
Facial recognition software may still not be used in the law enforcement initiative search and locate field to hunt down potential terrorists even if it is impeccable. Law enforcement may exploit people's concern about safety to amass more power which leads to mass surveillance. Additionally, even if law enforcement argues that they use the data under the strict policy, hackers or other malicious actors may breach the database to misuse the information.



I would prefer the screening done by an elaborate algorithm, and if I consider the results to be unfair or biased I would apply for humans to scrutinize. The human resources algorithm may make decisions more independent and have quick feedback, and such an algorithm could save a lot of time to wait. Additionally, traditional human resources that operate by people may have a variety of preferences and discriminations. A lot of important decisions made by humans in history are inconsistent, biased, and phenomenally terrible. However, the human resources algorithm may exacerbate the present bias because they were designed and learned from human data. Moreover, the algorithm itself lacks the ability to individualize as humans.
I argue that the whole hiring process could be conducted via a human resources algorithm, but only the algorithm with transparency and accountability. For instance, the human actor often arbitrarily use their unregulated powers to decide which people to hire or not. They may decide to hire a person who has the same background as themselves or the same college. An example of my country is a highly confidential department stuffed with people who are relative to each other. Because of the security considerations, the department has the right to check the background of applicants. The consequence is all the people they hire are people who have connections, and they claim such people are hired for loyal and reliable reasons. This is a case of discretion to deliver individualized decisions is abused. A human judge could subtly eliminate the potential applicants with a faux-reasonable tone but the authentic factors may be morrally troubling and unethical such as race, gender, and other minor reasons like their bad mood or delicious lunch. According to their personal preference, they may exclude some capable applicants who are from specific backgrounds, impairing the interests of the company and applicants eventually.


Why Companies Like Uber Get Away With Bad Behavior
https://www.nytimes.com/2017/06/13/opinion/travis-kalanick-uber-bad-behavior.html

From the article, we can realize the astonishing fact that investors tend to forgive inexcusable behavior from executives if the companies are thriving, and financially successful. Although Uber struggles with countless lawsuits and notorious maltreatment towards its employees, contractors, and competitors, the creator of the toxic culture could come back to reign eventually. 
The corporate hustling culture causes some ethical issues. From the Deontology perspective, the unethical company's cultural values violate the foundation of human society's common sense. It is inappropriate to use "Always Be Hustlin" as an official core value. And the creators always have impunity because of their contributions, which should not be allowed. From the Utilitarianism perspective, such a malicious company's culture cultivates ruthless products that relentlessly exploit every drop of blood of its employees and contractors. The only ones who are beneficial are the morbid executives who are pleased to see people suffer. For the greater good of modern society's benefits, we need urgent action by policymakers, investors, researchers, and people who could stop the toxic company values prevalent. We should regularize the executives' improper behavior to create healthier, credible company culture environments for the future.

James Damore has sued Google. His infamous memo on women in tech is still nonsense
https://www.vox.com/the-big-idea/2017/8/11/16130452/google-memo-women-tech-biology-sexism

From the article, the most compelling issue must be the reasonable rebuttal of the so-called gender average difference. James Damore, the author of the infamous memo, was fired by Google and argues that the honest discussion about gender biases is being silenced by mainstream ideology. The article disproves the conclusion of the memo that the average woman's deficiencies and questions the memo's rhetorical strategy.
The notorious memo of James Damore is undoubtedly unethical. In his gender theory, human society should have a social division of labor. Such division of labor excludes half of humanity from working in the technology field, which would profoundly destroy the foundation of human prosperity. The embedded gender bias would lead to gender diversity benefits fading. To maximize the potential gender diversity benefits such as financial performance, team dynamics, and organizational effectiveness, we should encourage society to abandon gender bias.




The first ethical issue that concerned me most is racial discrimination caused by the facial-generating algorithm. The synthetic faces generated by the generative artificial intelligence demonstrate the preference toward white people by whom artificial intelligence research is dominated. Such a generating algorithm reflects the implicit biases of the machine learning field. However, people who advocate the algorithm argue that selected data and adjusted algorithms may lose their potential. The problem may turn to weighing between the velocity of the machine learning development and the implicit racial bias among the facial-generating algorithm. The creator of the algorithm could take precautions such as using more representative data to prevent the system from impairing the interest of certain groups, but the future of the algorithm would be fuzzy.
The second ethical issue is misinformation and harmful content produced by generative artificial intelligence. Such misinformation could manipulate human preferences, cause conflict among countries, and even be disastrous for democratic systems. Malicious actors or foreign espionage agencies may exploit the forged images or videos to slander a formidable politician.

To rein the ethical issue of facial-generating algorithms we need a systematic revolution in the generative artificial intelligence field. The creator of the facial-generating algorithm may replace the data with more "fair" data, but such data is still not complete, balanced, or selected appropriately. Additionally, the source of algorithmic racial bias does not simply come from the dataset but also comes from the algorithm itself and its creators. The creator of the facial-generating algorithm may unconsciously allocate too much weight to the model of specific ethnic groups. The data itself may be unbiased, but such ethnic affinities algorithm may select specific white faces that it generates. After the racial bias algorithm is developed and refined over the years, it may amplify the bias. So the primary factor of the ethical problem solution is to balance the algorithm's preference. The team of facial-generating algorithm creators must be diverse to avoid such a white domine situation, and the algorithm must design accountability and fairness. Such alter in the facial-generating algorithm may cause technical regress, but the mitigation of racial discrimination benefit is huge and uncountable. So we have to extract the embedding of racial bias in the machine learning field. The incremental racial bias in algorithms may turn into social division in the future. We may sacrifice the present benefit and convenience of technology to achieve a diverse and prolific tech future.
